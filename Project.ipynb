{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db57db20",
   "metadata": {},
   "source": [
    "#### COMP8420 - Advanced Natural Language Processing\n",
    "#### Session 1, 2024\n",
    "# Major Project\n",
    "---\n",
    "\n",
    "## Revolutionizing IT Support with an AI- Powered Chatbot\n",
    "\n",
    "#### Leveraging NLP for Efficient Ticket Triage and Resolution\n",
    "___\n",
    "\n",
    "#### Group Members:\n",
    "\n",
    "### Mir Sadia Afrin - 47965495\n",
    "### Fursan Afzal - 47748893\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db031d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mirayon/anaconda3/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8150d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mirayon/anaconda3/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in /Users/mirayon/anaconda3/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mirayon/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d24aa",
   "metadata": {},
   "source": [
    "## Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202d8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6b149",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0168d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>question about cancelling order {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've understood you have a question regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BQZ</td>\n",
       "      <td>i have a question about cancelling oorder {{Or...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've been informed that you have a question ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLQZ</td>\n",
       "      <td>i need help cancelling puchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I can sense that you're seeking assistance wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BL</td>\n",
       "      <td>I need to cancel purchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I understood that you need assistance with can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCELN</td>\n",
       "      <td>I cannot afford this order, cancel purchase {{...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I'm sensitive to the fact that you're facing f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flags                                        instruction category  \\\n",
       "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
       "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
       "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
       "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
       "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
       "\n",
       "         intent                                           response  \n",
       "0  cancel_order  I've understood you have a question regarding ...  \n",
       "1  cancel_order  I've been informed that you have a question ab...  \n",
       "2  cancel_order  I can sense that you're seeking assistance wit...  \n",
       "3  cancel_order  I understood that you need assistance with can...  \n",
       "4  cancel_order  I'm sensitive to the fact that you're facing f...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "\n",
    "# Inspect the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6561dc08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    # Implement text cleaning steps\n",
    "    return text\n",
    "\n",
    "df['instruction'] = df['instruction'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_df['instruction'].tolist())\n",
    "train_labels = tokenize_function(train_df['response'].tolist())\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b266b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0f931e11b24a38935426ff5f6e2604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7942, 'grad_norm': 23.240041732788086, 'learning_rate': 4.8449612403100775e-05, 'epoch': 0.09}\n",
      "{'loss': 4.3643, 'grad_norm': 11.681827545166016, 'learning_rate': 4.6899224806201553e-05, 'epoch': 0.19}\n",
      "{'loss': 4.2739, 'grad_norm': 8.936216354370117, 'learning_rate': 4.5348837209302326e-05, 'epoch': 0.28}\n",
      "{'loss': 4.1199, 'grad_norm': 18.024036407470703, 'learning_rate': 4.3798449612403104e-05, 'epoch': 0.37}\n",
      "{'loss': 4.0832, 'grad_norm': 5.541406154632568, 'learning_rate': 4.2248062015503877e-05, 'epoch': 0.47}\n",
      "{'loss': 4.0011, 'grad_norm': 4.467293739318848, 'learning_rate': 4.0697674418604655e-05, 'epoch': 0.56}\n",
      "{'loss': 3.9861, 'grad_norm': 19.31856918334961, 'learning_rate': 3.914728682170543e-05, 'epoch': 0.65}\n",
      "{'loss': 3.9742, 'grad_norm': 6.0319504737854, 'learning_rate': 3.7596899224806207e-05, 'epoch': 0.74}\n",
      "{'loss': 3.9584, 'grad_norm': 7.928410053253174, 'learning_rate': 3.604651162790698e-05, 'epoch': 0.84}\n",
      "{'loss': 3.8854, 'grad_norm': 4.654712200164795, 'learning_rate': 3.449612403100775e-05, 'epoch': 0.93}\n",
      "{'loss': 3.8851, 'grad_norm': 7.61992883682251, 'learning_rate': 3.294573643410852e-05, 'epoch': 1.02}\n",
      "{'loss': 3.8493, 'grad_norm': 4.184144496917725, 'learning_rate': 3.13953488372093e-05, 'epoch': 1.12}\n",
      "{'loss': 3.8238, 'grad_norm': 8.22586727142334, 'learning_rate': 2.9844961240310077e-05, 'epoch': 1.21}\n",
      "{'loss': 3.8302, 'grad_norm': 3.797067165374756, 'learning_rate': 2.8294573643410853e-05, 'epoch': 1.3}\n",
      "{'loss': 3.8253, 'grad_norm': 9.313591957092285, 'learning_rate': 2.674418604651163e-05, 'epoch': 1.4}\n",
      "{'loss': 3.8333, 'grad_norm': 4.922866344451904, 'learning_rate': 2.5193798449612404e-05, 'epoch': 1.49}\n",
      "{'loss': 3.8055, 'grad_norm': 4.23712158203125, 'learning_rate': 2.364341085271318e-05, 'epoch': 1.58}\n",
      "{'loss': 3.8259, 'grad_norm': 4.065124988555908, 'learning_rate': 2.2093023255813955e-05, 'epoch': 1.67}\n",
      "{'loss': 3.8397, 'grad_norm': 3.7443795204162598, 'learning_rate': 2.054263565891473e-05, 'epoch': 1.77}\n",
      "{'loss': 3.7916, 'grad_norm': 3.1133768558502197, 'learning_rate': 1.8992248062015506e-05, 'epoch': 1.86}\n",
      "{'loss': 3.7615, 'grad_norm': 5.3749470710754395, 'learning_rate': 1.744186046511628e-05, 'epoch': 1.95}\n",
      "{'loss': 3.8032, 'grad_norm': 4.418206691741943, 'learning_rate': 1.5891472868217057e-05, 'epoch': 2.05}\n",
      "{'loss': 3.7343, 'grad_norm': 5.145410060882568, 'learning_rate': 1.434108527131783e-05, 'epoch': 2.14}\n",
      "{'loss': 3.757, 'grad_norm': 5.181374549865723, 'learning_rate': 1.2790697674418606e-05, 'epoch': 2.23}\n",
      "{'loss': 3.7164, 'grad_norm': 5.17976188659668, 'learning_rate': 1.1240310077519382e-05, 'epoch': 2.33}\n",
      "{'loss': 3.7742, 'grad_norm': 2.7446463108062744, 'learning_rate': 9.689922480620156e-06, 'epoch': 2.42}\n",
      "{'loss': 3.7567, 'grad_norm': 4.627258777618408, 'learning_rate': 8.139534883720931e-06, 'epoch': 2.51}\n",
      "{'loss': 3.7332, 'grad_norm': 4.034471035003662, 'learning_rate': 6.589147286821707e-06, 'epoch': 2.6}\n",
      "{'loss': 3.7441, 'grad_norm': 7.044722557067871, 'learning_rate': 5.0387596899224804e-06, 'epoch': 2.7}\n",
      "{'loss': 3.7333, 'grad_norm': 5.516432762145996, 'learning_rate': 3.488372093023256e-06, 'epoch': 2.79}\n",
      "{'loss': 3.7786, 'grad_norm': 4.084080219268799, 'learning_rate': 1.937984496124031e-06, 'epoch': 2.88}\n",
      "{'loss': 3.7432, 'grad_norm': 12.302135467529297, 'learning_rate': 3.8759689922480623e-07, 'epoch': 2.98}\n",
      "{'train_runtime': 59376.9554, 'train_samples_per_second': 1.086, 'train_steps_per_second': 0.272, 'train_loss': 3.8980842739371364, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16125, training_loss=3.8980842739371364, metrics={'train_runtime': 59376.9554, 'train_samples_per_second': 1.086, 'train_steps_per_second': 0.272, 'total_flos': 4212746108928000.0, 'train_loss': 3.8980842739371364, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81285720",
   "metadata": {},
   "source": [
    "## Evaluating the Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12908898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1175d21310cd49c6a8e53ce45a4c4894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7349560260772705, 'eval_runtime': 566.5196, 'eval_samples_per_second': 9.488, 'eval_steps_per_second': 1.186, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenize_function(test_df['instruction'].tolist())\n",
    "test_labels = tokenize_function(test_df['response'].tolist())\n",
    "\n",
    "test_dataset = CustomDataset(test_encodings, test_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c398e",
   "metadata": {},
   "source": [
    "## Creating Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48f03e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Select the appropriate device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84c5a7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d58591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I want a refund for my recent purchase.\n",
      "Response: I want a refund for my recent purchase.\n",
      "\n",
      "Rated 5 out of 5 by Anonymous from Great product! I bought this product because I was looking for a way to get a better quality product. I have been using it for about a month now and it works great. The only thing I would change is the size of the tip. It is a little smaller than the one on the back of my phone, but it is still a good size for me. If you have any questions, please don't hesitate to call us. We will be happy to help you with any issues you may have.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(prompt, model, tokenizer, max_length=150):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move the input IDs and attention mask to the correct device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Ensure max_length does not exceed model's max position embeddings\n",
    "    max_length = min(max_length, model.config.n_positions)\n",
    "    \n",
    "    # Generate a response using beam search\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_beams=5,              # Use beam search with 5 beams\n",
    "        early_stopping=True,      # Stop when at least num_beams sentences are finished per batch\n",
    "        no_repeat_ngram_size=2,   # Prevents repeating n-grams\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the function\n",
    "query = \"I want a refund for my recent purchase.\"\n",
    "response = generate_response(query, model, tokenizer)\n",
    "print(f\"Query: {query}\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c74d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I can't talk with  a human agent\n",
      "Expected Response: Thank you for reaching out! I'm sorry to hear that you're facing difficulties in connecting with a human agent. Please rest assured that I'm here to assist you and resolve this issue. Could you please provide me with some more details about the problem you're experiencing? This will help me find the best solution for you.\n",
      "Generated Response: I can't talk with  a human agent, but I can tell you that I'm not going to lie to you. I don't know what you're talking about.\"\n",
      "\n",
      "\"I'm sorry,\" she said. \"I just want you to know that this is not the first time I've heard you say something like that. It's not like you've ever done anything like this before. You're just a normal human being, you know? I mean, I know it's hard to believe that you'd say that to someone like me. But I have to admit, it was hard for me when I first met you, because I didn't really know you at all, and I just wanted to make sure you were okay.\n",
      "--------------------------------------------------\n",
      "Prompt: I have got to locate hte bills from {{Person Name}}\n",
      "Expected Response: I'll get right on it! I understand your need to locate the bills associated with {{Person Name}}. To assist you in finding the relevant invoices, could you please provide me with some additional details such as the date range or any other specific information that might help narrow down the search? Rest assured, I'm here to guide you every step of the way and ensure a seamless experience.\n",
      "Generated Response: I have got to locate hte bills from {{Person Name}}. {{Title}} is the name of the bill you are looking for.\n",
      "\n",
      "If you don't have a bill, you can find it by clicking on the {{Name}} link in the top right corner of your browser. You can also search for bills by using the search box at the bottom of this page. If you're not sure what to look for, just click on a name and you'll get a list of all the bills you've found.\n",
      "--------------------------------------------------\n",
      "Prompt: I cannot pay, help me to inform of a problem with payments\n",
      "Expected Response: I apologize for the inconvenience caused by the payment issue you're experiencing. I understand how frustrating it can be when you encounter difficulties while trying to make a payment. Rest assured, I'm here to assist you in resolving this problem. To help us further investigate and address the issue, could you please provide more details about the problem you are encountering? Any specific error messages or steps leading up to the issue would be helpful. Your input is valuable, and we're committed to finding a solution together. Thank you for bringing this to our attention, and we'll work diligently to get it sorted.\n",
      "Generated Response: I cannot pay, help me to inform of a problem with payments,\" he said.\n",
      "\n",
      "\"I am not going to be able to do that. I have to go to the bank and get my money back. If I don't get it back, I will be in jail.\"\n",
      "--------------------------------------------------\n",
      "Prompt: I want help speaking to customer service\n",
      "Expected Response: Grateful for your contact! I get the sense that you're looking for assistance in speaking to our customer service team. I'm here to guide you through the process and ensure that you have a seamless experience. To connect with our customer service, you can reach them during {{Customer Support Hours}}. If you have any other questions, concerns, or experiences you'd like to share, please feel free to let me know. I'm here to help!\n",
      "Generated Response: I want help speaking to customer service. If you have any questions, feel free to contact us.\n",
      "--------------------------------------------------\n",
      "Prompt: I try to see th accepted payment options\n",
      "Expected Response: Thank you for reaching out! I completely understand that you tried to access the accepted payment options on our platform. I apologize for any inconvenience caused. Allow me to assist you in viewing our available payment methods.\n",
      "\n",
      "To check the accepted payment options, you can visit our website's \"Payment\" or \"Checkout\" page. There, you will find a comprehensive list of the various payment methods we accept. If you encounter any difficulties or have further questions, please don't hesitate to let me know.\n",
      "\n",
      "Your satisfaction is our priority, and I'm here to ensure a seamless payment experience for you.\n",
      "Generated Response: I try to see th accepted payment options.\n",
      "\n",
      "If you have any questions, please feel free to contact us.\n",
      "--------------------------------------------------\n",
      "Prompt: where to sign up to the company nmewsletter\n",
      "Expected Response: Thank you for your interest in signing up for our company newsletter! To subscribe, you can visit our website at {{Website URL}} and navigate to the newsletter section. There you will find a sign-up form where you can enter your email address and other required information. Once you submit the form, you will be successfully subscribed to our newsletter and receive regular updates directly in your inbox. If you require any further assistance or have any questions, please don't hesitate to let me know.\n",
      "Generated Response: where to sign up to the company nmewsletter.com.\n",
      "--------------------------------------------------\n",
      "Prompt: I'd like to see the withdrwaal fee how can i do it\n",
      "Expected Response: I'll do my best! To view the withdrawal fee, you can log in to your account and navigate to the \"Fee Schedule\" or \"Account Charges\" section. This will provide you with the details of the withdrawal fee and any associated charges.\n",
      "Generated Response: I'd like to see the withdrwaal fee how can i do it?\"\n",
      "\n",
      "\"I don't know what you're talking about. I'm not sure if it's a good idea or not, but I think it would be nice if you could give me some pointers on how to use it. If you have any suggestions, I'd love to hear them. Thanks for your time, and I'll see you next time.\"\n",
      "--------------------------------------------------\n",
      "Prompt: I want to speak with someone\n",
      "Expected Response: I see what you mean! I'm on the same page that you would like to speak with someone. Your request is important, and I'm here to help. Let me quickly connect you with a human representative who will be able to assist you further. Kindly stay on the line while I transfer you.\n",
      "Generated Response: I want to speak with someone who has been involved in this,\" he said.\n",
      "\n",
      "\"I think it's important for us to be able to talk about what's going on in our community and what we can do to make sure that we're doing the right thing for the community.\"\n",
      "--------------------------------------------------\n",
      "Prompt: can you help me getting bill #85632?\n",
      "Expected Response: I'll take care of it! I completely understand your request to access the bill with the number #{{Invoice Number}}. Please allow me a moment to retrieve the bill for you. In the meantime, if you have any other questions or concerns, feel free to let me know. I'm here to assist you!\n",
      "Generated Response: can you help me getting bill #85632?\n",
      "\n",
      "If you have any questions, please feel free to reach out to me on Twitter or email me at [email protected]\n",
      "--------------------------------------------------\n",
      "Prompt: I don't know how to take a quick look at invoice #12588\n",
      "Expected Response: I've realized you're unsure about how to quickly access the details of invoice #{{Invoice Number}}. Allow me to guide you through the process. To take a quick look at your invoice, you can log in to your account on our website and navigate to the \"Billing\" or \"Invoice\" section. There, you should be able to find a list of your invoices, including invoice #{{Invoice Number}}. Clicking on the invoice will provide you with a detailed view of the charges, due date, and any other relevant information. If you encounter any difficulties during this process, please don't hesitate to reach out to our customer support team, who will be happy to assist you further.\n",
      "Generated Response: I don't know how to take a quick look at invoice #12588, but I'm sure you can figure it out for yourself.\n",
      "\n",
      "If you have any questions, please feel free to contact me. I'll be happy to help.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(prompt, model, tokenizer, max_length=150):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move the input IDs and attention mask to the correct device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Ensure max_length does not exceed model's max position embeddings\n",
    "    max_length = min(max_length, model.config.n_positions)\n",
    "    \n",
    "    # Generate a response using beam search\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_beams=5,               # Use beam search with 5 beams\n",
    "        early_stopping=True,       # Stop when at least num_beams sentences are finished per batch\n",
    "        no_repeat_ngram_size=2,    # Prevents repeating n-grams\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "\n",
    "# Sample a subset for testing\n",
    "test_sample = df.sample(10, random_state=42)\n",
    "\n",
    "# Function to test the response generation\n",
    "def test_response_generation(test_sample, model, tokenizer):\n",
    "    results = []\n",
    "    for idx, row in test_sample.iterrows():\n",
    "        prompt = row['instruction']  # Use instruction column as prompt\n",
    "        expected_response = row['response']\n",
    "        generated_response = generate_response(prompt, model, tokenizer)\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'expected_response': expected_response,\n",
    "            'generated_response': generated_response\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the test\n",
    "test_results = test_response_generation(test_sample, model, tokenizer)\n",
    "\n",
    "# Display the results\n",
    "for idx, row in test_results.iterrows():\n",
    "    print(f\"Prompt: {row['prompt']}\")\n",
    "    print(f\"Expected Response: {row['expected_response']}\")\n",
    "    print(f\"Generated Response: {row['generated_response']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaba388",
   "metadata": {},
   "source": [
    "## Integrate Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6769a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our refund policy states that you can request a refund within 30 days of purchase...\n"
     ]
    }
   ],
   "source": [
    "knowledge_base = {\n",
    "    \"refund_policy\": \"Our refund policy states that you can request a refund within 30 days of purchase...\",\n",
    "}\n",
    "\n",
    "def generate_response_with_kb(prompt):\n",
    "    if \"refund policy\" in prompt.lower():\n",
    "        return knowledge_base[\"refund_policy\"]\n",
    "    else:\n",
    "        return generate_response(prompt)\n",
    "\n",
    "# Test the function with knowledge base integration\n",
    "query = \"What is your refund policy?\"\n",
    "response = generate_response_with_kb(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd869e",
   "metadata": {},
   "source": [
    "## Implement Escalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4db7397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query has been escalated to a human agent. Please wait for a response.\n"
     ]
    }
   ],
   "source": [
    "def escalate_to_human(prompt):\n",
    "    # Logic to escalate to human agent\n",
    "    return \"Your query has been escalated to a human agent. Please wait for a response.\"\n",
    "\n",
    "def handle_query(prompt):\n",
    "    if \"complex issue\" in prompt:  # Placeholder for more complex logic\n",
    "        return escalate_to_human(prompt)\n",
    "    else:\n",
    "        return generate_response_with_kb(prompt)\n",
    "\n",
    "# Test the complete system\n",
    "query = \"I have a complex issue with my account.\"\n",
    "response = handle_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cedf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    # Encode the prompt into tokens\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Check if the inputs tensor is empty\n",
    "    if inputs.numel() == 0:\n",
    "        return \"Please provide a valid prompt.\"\n",
    "    \n",
    "    inputs = inputs.to(model.device)  # Move input to the same device as the model\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d0292e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    # Encode the prompt into tokens\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Check if the inputs tensor is empty\n",
    "    if inputs.numel() == 0:\n",
    "        return \"Please provide a valid prompt.\"\n",
    "    \n",
    "    inputs = inputs.to(model.device)  # Move input to the same device as the model\n",
    "    \n",
    "    # Generate an attention mask\n",
    "    attention_mask = torch.ones(inputs.shape, device=model.device)\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "090cd695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot_interface(model, tokenizer):\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        response = generate_response(prompt, model, tokenizer)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot interface\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interface(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf18ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
